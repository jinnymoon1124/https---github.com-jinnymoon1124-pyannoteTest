from pyannote.audio import Pipeline
import psutil
import GPUtil
import time
import threading
import os
import whisper
import librosa
import soundfile as sf
import pickle
import json
import numpy as np
from datetime import datetime
from collections import defaultdict
from sklearn.metrics.pairwise import cosine_similarity

def monitor_resources(stop_event):
    """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ë¥¼ ëª¨ë‹ˆí„°ë§í•˜ëŠ” í•¨ìˆ˜"""
    process = psutil.Process(os.getpid())
    
    while not stop_event.is_set():
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB ë‹¨ìœ„)
        memory_info = process.memory_info()
        memory_mb = memory_info.rss / 1024 / 1024
        
        # CPU ì‚¬ìš©ë¥ 
        cpu_percent = process.cpu_percent()
        
        # GPU ì‚¬ìš©ëŸ‰ í™•ì¸
        gpus = GPUtil.getGPUs()
        if gpus:
            gpu = gpus[0]  # ì²« ë²ˆì§¸ GPU ì‚¬ìš©
            gpu_memory_used = gpu.memoryUsed
            gpu_memory_total = gpu.memoryTotal
            gpu_utilization = gpu.load * 100
            gpu_temp = gpu.temperature if hasattr(gpu, 'temperature') else 'N/A'
            print(f"[ë¦¬ì†ŒìŠ¤] ë©”ëª¨ë¦¬: {memory_mb:.1f}MB | CPU: {cpu_percent:.1f}% | GPU ë©”ëª¨ë¦¬: {gpu_memory_used}MB/{gpu_memory_total}MB | GPU ì‚¬ìš©ë¥ : {gpu_utilization:.1f}% | GPU ì˜¨ë„: {gpu_temp}Â°C")
        else:
            print(f"[ë¦¬ì†ŒìŠ¤] ë©”ëª¨ë¦¬: {memory_mb:.1f}MB | CPU: {cpu_percent:.1f}% | GPU: ê°ì§€ë˜ì§€ ì•ŠìŒ")
        
        time.sleep(2)  # 2ì´ˆë§ˆë‹¤ ëª¨ë‹ˆí„°ë§

def extract_and_save_embeddings(pipeline, diarization, audio_file, results, target_speakers=None):
    """
    í™”ìë³„ ì„ë² ë”© ë²¡í„°ë¥¼ ì¶”ì¶œí•˜ê³  ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    target_speakers: ì €ì¥í•  ëŒ€ìƒ í™”ì ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ëª¨ë“  í™”ì)
    """
    if target_speakers:
        print(f"\n=== ìƒˆë¡œìš´ í™”ì ì„ë² ë”© ì¶”ì¶œ ë° ì €ì¥ ({', '.join(target_speakers)}) ===")
    else:
        print("\n=== í™”ì ì„ë² ë”© ì¶”ì¶œ ë° ì €ì¥ ===")
    
    # ì„ë² ë”© ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    embeddings_dir = "embeddings"
    os.makedirs(embeddings_dir, exist_ok=True)
    
    # í™”ìë³„ ì„ë² ë”© ì €ì¥ì†Œ
    speaker_embeddings = defaultdict(list)
    
    try:
        # ì„ë² ë”© ëª¨ë¸ ì¶”ì¶œ (íŒŒì´í”„ë¼ì¸ ë‚´ë¶€ì˜ ì„ë² ë”© ëª¨ë¸ ì ‘ê·¼)
        embedding_model = pipeline._embedding
        print(f"ì„ë² ë”© ëª¨ë¸: {type(embedding_model)}")
        
        # ì˜¤ë””ì˜¤ ë°ì´í„° ë¡œë“œ
        audio_data, sample_rate = librosa.load(audio_file, sr=16000)
        current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        print(f"ì„ë² ë”© ì¶”ì¶œ ì‹œì‘... (ì´ {len(results)}ê°œ ì„¸ê·¸ë¨¼íŠ¸)")
        
        for idx, result in enumerate(results):
            speaker = result["speaker"]
            start_time = result["start"]
            end_time = result["end"]
            
            # ëŒ€ìƒ í™”ì í•„í„°ë§ (target_speakersê°€ ì§€ì •ëœ ê²½ìš°)
            if target_speakers and speaker not in target_speakers:
                continue
            
            # í•´ë‹¹ êµ¬ê°„ì˜ ì˜¤ë””ì˜¤ ì¶”ì¶œ
            start_sample = int(start_time * sample_rate)
            end_sample = int(end_time * sample_rate)
            segment_audio = audio_data[start_sample:end_sample]
            
            # ë„ˆë¬´ ì§§ì€ ì„¸ê·¸ë¨¼íŠ¸ëŠ” ê±´ë„ˆë›°ê¸° (0.5ì´ˆ ë¯¸ë§Œ)
            if len(segment_audio) < sample_rate * 0.5:
                print(f"ì„¸ê·¸ë¨¼íŠ¸ ë„ˆë¬´ ì§§ìŒ, ê±´ë„ˆë›°ê¸°: {speaker} ({start_time:.1f}s-{end_time:.1f}s)")
                continue
            
            try:
                # ì§ì ‘ WeSpeaker ëª¨ë¸ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ì¶”ì¶œ (íŒŒì¼ ì—†ì´ ë©”ëª¨ë¦¬ì—ì„œ ì§ì ‘)
                from pyannote.audio import Model
                
                # ë‚´ë¶€ ëª¨ë¸ì— ì§ì ‘ ì ‘ê·¼ ë˜ëŠ” ìƒˆë¡œ ë¡œë“œ
                if hasattr(embedding_model, 'model_'):
                    direct_model = embedding_model.model_
                else:
                    # WeSpeaker ëª¨ë¸ ì§ì ‘ ë¡œë“œ
                    model_path = "models/pyannote_model_wespeaker-voxceleb-resnet34-LM.bin"
                    direct_model = Model.from_pretrained(model_path)
                    if torch.cuda.is_available():
                        direct_model = direct_model.cuda()
                
                # ì˜¬ë°”ë¥¸ í…ì„œ ì°¨ì›ìœ¼ë¡œ ë³€í™˜: (batch, channels, samples)
                audio_tensor = torch.from_numpy(segment_audio).float()
                audio_tensor = audio_tensor.unsqueeze(0).unsqueeze(0)  # (1, 1, samples)
                
                if torch.cuda.is_available():
                    audio_tensor = audio_tensor.cuda()
                
                # ì„ë² ë”© ì¶”ì¶œ
                with torch.no_grad():
                    embedding = direct_model(audio_tensor)
                
                # GPU tensorë¥¼ CPU numpyë¡œ ë³€í™˜
                embedding_vector = embedding.cpu().numpy().flatten()
                
                # í™”ìë³„ ì„ë² ë”© ì €ì¥
                speaker_embeddings[speaker].append({
                    'embedding': embedding_vector,
                    'start_time': start_time,
                    'end_time': end_time,
                    'duration': end_time - start_time,
                    'text': result['text'],
                    'filename': result['filename'],
                    'timestamp': current_time
                })
                
                print(f"[{idx+1:2d}/{len(results)}] ì„ë² ë”© ì¶”ì¶œ: {speaker} ({start_time:.1f}s-{end_time:.1f}s) - ì°¨ì›: {embedding_vector.shape}")
                
            except Exception as e:
                print(f"ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨ ({speaker}, {idx}): {e}")
        
        # í™”ìë³„ ì„ë² ë”© ë°ì´í„° ì €ì¥
        print(f"\nì„ë² ë”© ë°ì´í„° ì €ì¥ ì¤‘...")
        total_embeddings = 0
        
        for speaker, embeddings in speaker_embeddings.items():
            if not embeddings:
                continue
                
            # ê°œë³„ ì„ë² ë”© íŒŒì¼ ì €ì¥
            embedding_file = os.path.join(embeddings_dir, f"{current_time}_{speaker}_embeddings.pkl")
            with open(embedding_file, 'wb') as f:
                pickle.dump(embeddings, f)
            
            # í‰ê·  ì„ë² ë”© ê³„ì‚° (í™”ì ëŒ€í‘œ ë²¡í„°)
            all_embeddings = np.array([emb['embedding'] for emb in embeddings])
            mean_embedding = np.mean(all_embeddings, axis=0)
            std_embedding = np.std(all_embeddings, axis=0)
            
            # í™”ì í”„ë¡œíŒŒì¼ ì €ì¥
            speaker_profile = {
                'speaker_id': speaker,
                'mean_embedding': mean_embedding,
                'std_embedding': std_embedding,
                'num_segments': len(embeddings),
                'total_duration': sum([emb['duration'] for emb in embeddings]),
                'timestamp': current_time,
                'audio_file': audio_file,
                'embedding_dim': mean_embedding.shape[0],
                'sample_embeddings': embeddings[:3] if len(embeddings) > 3 else embeddings  # ìƒ˜í”Œ ì €ì¥
            }
            
            profile_file = os.path.join(embeddings_dir, f"{current_time}_{speaker}_profile.pkl")
            with open(profile_file, 'wb') as f:
                pickle.dump(speaker_profile, f)
            
            total_embeddings += len(embeddings)
            print(f"âœ… {speaker} í”„ë¡œíŒŒì¼ ì €ì¥: {profile_file}")
            print(f"   - ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜: {len(embeddings)}")
            print(f"   - ì´ ë°œí™” ì‹œê°„: {speaker_profile['total_duration']:.2f}ì´ˆ")
            print(f"   - ì„ë² ë”© ì°¨ì›: {mean_embedding.shape[0]}")
        
        # ì „ì²´ ì„¸ì…˜ ë©”íƒ€ë°ì´í„° ì €ì¥
        session_metadata = {
            'timestamp': current_time,
            'audio_file': audio_file,
            'speakers': list(speaker_embeddings.keys()),
            'total_segments': len(results),
            'total_embeddings': total_embeddings,
            'embedding_dim': mean_embedding.shape[0] if speaker_embeddings else 0,
            'sample_rate': sample_rate
        }
        
        metadata_file = os.path.join(embeddings_dir, f"{current_time}_session_metadata.json")
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(session_metadata, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ‰ ì„ë² ë”© ë°ì´í„° ì €ì¥ ì™„ë£Œ!")
        print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {embeddings_dir}/")
        print(f"ğŸ“Š ì´ ì„ë² ë”© ìˆ˜: {total_embeddings}ê°œ")
        print(f"ğŸ‘¥ í™”ì ìˆ˜: {len(speaker_embeddings)}ëª…")
        
        return speaker_embeddings, session_metadata
        
    except Exception as e:
        print(f"âŒ ì„ë² ë”© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {}, {}

def load_speaker_embeddings(embeddings_dir="embeddings"):
    """ì €ì¥ëœ í™”ì ì„ë² ë”© ë°ì´í„° ë¡œë“œ"""
    import glob
    
    profiles = {}
    profile_files = glob.glob(f"{embeddings_dir}/*_profile.pkl")
    
    print(f"\n=== ì €ì¥ëœ í™”ì í”„ë¡œíŒŒì¼ ë¡œë“œ ===")
    print(f"ë°œê²¬ëœ í”„ë¡œíŒŒì¼ íŒŒì¼: {len(profile_files)}ê°œ")
    
    for profile_file in profile_files:
        try:
            with open(profile_file, 'rb') as f:
                profile = pickle.load(f)
                speaker_id = profile['speaker_id']
                profiles[speaker_id] = profile
                print(f"âœ… ë¡œë“œ: {speaker_id} ({profile['num_segments']}ê°œ ì„¸ê·¸ë¨¼íŠ¸)")
        except Exception as e:
            print(f"âŒ í”„ë¡œíŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({profile_file}): {e}")
    
    return profiles

def identify_speaker(new_embedding, known_profiles, threshold=0.8):
    """ìƒˆë¡œìš´ ì„ë² ë”©ê³¼ ê¸°ì¡´ í™”ì í”„ë¡œíŒŒì¼ì„ ë¹„êµí•˜ì—¬ í™”ì ì‹ë³„"""
    if not known_profiles:
        return "UNKNOWN", 0.0
    
    best_match = None
    best_similarity = 0
    
    # ì…ë ¥ ì„ë² ë”©ì„ 2D ë°°ì—´ë¡œ ë³€í™˜ (cosine_similarity ìš”êµ¬ì‚¬í•­)
    if new_embedding.ndim == 1:
        new_embedding = new_embedding.reshape(1, -1)
    
    for speaker_id, profile in known_profiles.items():
        mean_embedding = profile['mean_embedding'].reshape(1, -1)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarity = cosine_similarity(new_embedding, mean_embedding)[0][0]
        
        if similarity > best_similarity:
            best_similarity = similarity
            best_match = speaker_id
    
    if best_similarity > threshold:
        return best_match, best_similarity
    else:
        return "UNKNOWN", best_similarity

def verify_and_selective_embedding_save(pipeline, diarization, audio_file, results):
    """
    í™”ì ì‚¬ì „ ê²€ì¦ í›„ ìƒˆë¡œìš´ í™”ìì— ëŒ€í•´ì„œë§Œ ì„ë² ë”© ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    1ë‹¨ê³„: ë¹ ë¥¸ ê²€ì¦ìš© ì„ë² ë”© ì¶”ì¶œ (ëŒ€í‘œ ì„¸ê·¸ë¨¼íŠ¸ë§Œ)
    2ë‹¨ê³„: ê¸°ì¡´ í™”ìì™€ ë¹„êµí•˜ì—¬ ìƒˆë¡œìš´ í™”ì íŒë³„
    3ë‹¨ê³„: ìƒˆë¡œìš´ í™”ìì— ëŒ€í•´ì„œë§Œ ì „ì²´ ì„ë² ë”© ì¶”ì¶œ ë° ì €ì¥
    """
    print("ğŸ” í™”ì ì‚¬ì „ ê²€ì¦ ì‹œì‘...")
    
    # ê¸°ì¡´ í™”ì í”„ë¡œíŒŒì¼ ë¡œë“œ
    import glob
    existing_profiles = {}
    
    try:
        # ê¸°ì¡´ í”„ë¡œíŒŒì¼ íŒŒì¼ë“¤ ë¡œë“œ
        profile_files = glob.glob("embeddings/*_profile.pkl")
        
        for profile_file in profile_files:
            try:
                with open(profile_file, 'rb') as f:
                    profile = pickle.load(f)
                    speaker_id = profile['speaker_id']
                    existing_profiles[speaker_id] = profile
                    print(f"âœ… ê¸°ì¡´ í™”ì ë¡œë“œ: {speaker_id} ({profile['num_segments']}ê°œ ì„¸ê·¸ë¨¼íŠ¸)")
            except Exception as e:
                print(f"âŒ í”„ë¡œíŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({profile_file}): {e}")
        
        if not existing_profiles:
            print("ğŸ“ ê¸°ì¡´ ë“±ë¡ëœ í™”ìê°€ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  í™”ìë¥¼ ìƒˆë¡œ ë“±ë¡í•©ë‹ˆë‹¤.")
            # ëª¨ë“  í™”ìì— ëŒ€í•´ ì„ë² ë”© ì €ì¥
            speaker_embeddings, session_metadata = extract_and_save_embeddings(pipeline, diarization, audio_file, results)
            
            # ìƒˆë¡œìš´ í™”ìë“¤ì— ëŒ€í•œ ê¸°ë³¸ ì •ë³´ ë°˜í™˜
            verified_speakers = {}
            for speaker_label in speaker_embeddings.keys():
                verified_speakers[speaker_label] = {
                    'identified_as': f"ìƒˆë¡œìš´_í™”ì_{speaker_label}",
                    'confidence': 'ì‹ ê·œë“±ë¡',
                    'similarity': 0.0,
                    'is_known': False
                }
            
            return verified_speakers, speaker_embeddings
        
        print(f"ğŸ” ì´ {len(existing_profiles)}ëª…ì˜ ê¸°ì¡´ í™”ìì™€ ì‚¬ì „ ë¹„êµ ì¤‘...")
        
        # 1ë‹¨ê³„: ê° í™”ìë³„ ëŒ€í‘œ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë¹ ë¥¸ ê²€ì¦
        current_speakers = {}  # {speaker_label: [segments]}
        for result in results:
            speaker = result["speaker"]
            if speaker not in current_speakers:
                current_speakers[speaker] = []
            current_speakers[speaker].append(result)
        
        verified_speakers = {}
        speakers_to_save = []  # ìƒˆë¡œìš´ í™”ìë“¤ë§Œ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        
        for current_speaker, speaker_segments in current_speakers.items():
            print(f"\nğŸ¯ {current_speaker} ì‚¬ì „ ê²€ì¦ ì¤‘...")
            
            # ê°€ì¥ ê¸´ ì„¸ê·¸ë¨¼íŠ¸ ì„ íƒ (ëŒ€í‘œì„±ì„ ìœ„í•´)
            longest_segment = max(speaker_segments, key=lambda x: x['end'] - x['start'])
            
            # ëŒ€í‘œ ì„¸ê·¸ë¨¼íŠ¸ë§Œìœ¼ë¡œ ì„ë² ë”© ì¶”ì¶œ
            representative_embedding = extract_single_segment_embedding(
                pipeline, audio_file, longest_segment
            )
            
            if representative_embedding is None:
                print(f"   âŒ ëŒ€í‘œ ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨")
                continue
            
            # ê¸°ì¡´ í™”ìë“¤ê³¼ ë¹„êµ
            best_match = None
            best_similarity = 0
            all_similarities = {}
            
            for existing_speaker, profile in existing_profiles.items():
                existing_mean = profile['mean_embedding']
                
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarity = cosine_similarity(
                    representative_embedding.reshape(1, -1), 
                    existing_mean.reshape(1, -1)
                )[0][0]
                
                all_similarities[existing_speaker] = similarity
                
                if similarity > best_similarity:
                    best_similarity = similarity
                    best_match = existing_speaker
            
            # ì„ê³„ê°’ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨ (0.6 ì‚¬ìš©)
            threshold = 0.6
            
            if best_similarity >= threshold:
                identified_as = best_match
                confidence_level = "ë†’ìŒ" if best_similarity >= 0.8 else "ë³´í†µ"
                is_known = True
                print(f"   âœ… ê¸°ì¡´ í™”ì ë§¤ì¹­: {best_match} (ìœ ì‚¬ë„: {best_similarity:.4f}, ì‹ ë¢°ë„: {confidence_level})")
                print(f"   ğŸ’¾ ì„ë² ë”© ì €ì¥ ìƒëµ - ê¸°ì¡´ í”„ë¡œí•„ ì¬ì‚¬ìš©")
            else:
                identified_as = f"ìƒˆë¡œìš´_í™”ì_{current_speaker}"
                confidence_level = "ìƒˆë¡œìš´í™”ì"
                is_known = False
                speakers_to_save.append(current_speaker)
                print(f"   â“ ìƒˆë¡œìš´ í™”ì: {identified_as} (ìµœê³  ìœ ì‚¬ë„: {best_similarity:.4f})")
                print(f"   ğŸ’¾ ì„ë² ë”© ì €ì¥ ì˜ˆì •")
            
            # ìƒìœ„ 3ê°œ ìœ ì‚¬ë„ ì¶œë ¥
            sorted_similarities = sorted(all_similarities.items(), key=lambda x: x[1], reverse=True)
            print(f"   ğŸ“Š ìœ ì‚¬ë„ ìˆœìœ„:")
            for i, (speaker, sim) in enumerate(sorted_similarities[:3], 1):
                status = "âœ…" if sim >= threshold else "âŒ"
                print(f"      {status} {i}ìœ„: {speaker} ({sim:.4f})")
            
            verified_speakers[current_speaker] = {
                'identified_as': identified_as,
                'confidence': confidence_level,
                'similarity': best_similarity,
                'is_known': is_known,
                'all_similarities': all_similarities
            }
        
        print(f"\nğŸ‰ ì‚¬ì „ ê²€ì¦ ì™„ë£Œ!")
        print(f"ğŸ“Š ê²€ì¦ ê²°ê³¼ ìš”ì•½:")
        known_count = sum(1 for v in verified_speakers.values() if v['is_known'])
        new_count = len(verified_speakers) - known_count
        print(f"   - ê¸°ì¡´ í™”ì: {known_count}ëª… (ì„ë² ë”© ì €ì¥ ìƒëµ)")
        print(f"   - ìƒˆë¡œìš´ í™”ì: {new_count}ëª… (ì„ë² ë”© ì €ì¥ ì§„í–‰)")
        
        # 2ë‹¨ê³„: ìƒˆë¡œìš´ í™”ìë“¤ì— ëŒ€í•´ì„œë§Œ ì „ì²´ ì„ë² ë”© ì €ì¥
        new_speaker_embeddings = {}
        
        if speakers_to_save:
            print(f"\nğŸ’¾ ìƒˆë¡œìš´ í™”ìë“¤ì— ëŒ€í•œ ì„ë² ë”© ì €ì¥ ì‹œì‘...")
            
            # ìƒˆë¡œìš´ í™”ìë“¤ì˜ ê²°ê³¼ë§Œ í•„í„°ë§
            new_speaker_results = [r for r in results if r['speaker'] in speakers_to_save]
            
            # ì„ë² ë”© ì¶”ì¶œ ë° ì €ì¥ (ìƒˆë¡œìš´ í™”ìë“¤ë§Œ)
            new_speaker_embeddings, session_metadata = extract_and_save_embeddings(
                pipeline, diarization, audio_file, new_speaker_results, speakers_to_save
            )
        
        return verified_speakers, new_speaker_embeddings
        
    except Exception as e:
        print(f"âŒ í™”ì ì‚¬ì „ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {}, {}

def extract_single_segment_embedding(pipeline, audio_file, segment_result):
    """ë‹¨ì¼ ì„¸ê·¸ë¨¼íŠ¸ì— ëŒ€í•œ ì„ë² ë”© ì¶”ì¶œ (ë¹ ë¥¸ ê²€ì¦ìš©)"""
    try:
        # ì„ë² ë”© ëª¨ë¸ ì¶”ì¶œ
        embedding_model = pipeline._embedding
        
        # ì˜¤ë””ì˜¤ ë°ì´í„° ë¡œë“œ
        audio_data, sample_rate = librosa.load(audio_file, sr=16000)
        
        start_time = segment_result["start"]
        end_time = segment_result["end"]
        
        # í•´ë‹¹ êµ¬ê°„ì˜ ì˜¤ë””ì˜¤ ì¶”ì¶œ
        start_sample = int(start_time * sample_rate)
        end_sample = int(end_time * sample_rate)
        segment_audio = audio_data[start_sample:end_sample]
        
        # ë„ˆë¬´ ì§§ì€ ì„¸ê·¸ë¨¼íŠ¸ëŠ” ì œì™¸
        if len(segment_audio) < sample_rate * 0.5:
            return None
        
        # ì„ë² ë”© ì¶”ì¶œ
        if hasattr(embedding_model, 'model_'):
            direct_model = embedding_model.model_
        else:
            from pyannote.audio import Model
            model_path = "models/pyannote_model_wespeaker-voxceleb-resnet34-LM.bin"
            direct_model = Model.from_pretrained(model_path)
            if torch.cuda.is_available():
                direct_model = direct_model.cuda()
        
        # í…ì„œ ë³€í™˜
        audio_tensor = torch.from_numpy(segment_audio).float()
        audio_tensor = audio_tensor.unsqueeze(0).unsqueeze(0)  # (1, 1, samples)
        
        if torch.cuda.is_available():
            audio_tensor = audio_tensor.cuda()
        
        # ì„ë² ë”© ì¶”ì¶œ
        with torch.no_grad():
            embedding = direct_model(audio_tensor)
        
        return embedding.cpu().numpy().flatten()
        
    except Exception as e:
        print(f"âŒ ë‹¨ì¼ ì„¸ê·¸ë¨¼íŠ¸ ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return None

print("=== í™”ì ë¶„ë¦¬ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ===")
print("ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

# ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘
stop_monitoring = threading.Event()
monitor_thread = threading.Thread(target=monitor_resources, args=(stop_monitoring,))
monitor_thread.daemon = True
monitor_thread.start()

# ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ ì¶œë ¥
initial_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024
print(f"ì´ˆê¸° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {initial_memory:.1f}MB")

try:
    # yaml ë¶ˆëŸ¬ì˜¤ê¸°
    print("íŒŒì´í”„ë¼ì¸ ë¡œë”© ì¤‘...")
    pipeline = Pipeline.from_pretrained("models/pyannote_diarization_config.yaml")
    
    # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ ë° íŒŒì´í”„ë¼ì¸ì„ GPUë¡œ ì´ë™
    import torch
    if torch.cuda.is_available():
        device = torch.device("cuda")
        pipeline.to(device)
        print(f"GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
        print(f"íŒŒì´í”„ë¼ì¸ì„ GPUë¡œ ì´ë™í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    
    # íŒŒì´í”„ë¼ì¸ ë¡œë”© í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    after_load_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024
    print(f"íŒŒì´í”„ë¼ì¸ ë¡œë”© í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {after_load_memory:.1f}MB (ì¦ê°€ëŸ‰: {after_load_memory - initial_memory:.1f}MB)")

    # ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
    audio_file = "audio/test.wav"  # í…ŒìŠ¤íŠ¸í•  ìŒì„± íŒŒì¼

    # í™”ì ë¶„ë¦¬ ì‹¤í–‰
    print("í™”ì ë¶„ë¦¬ ì²˜ë¦¬ ì¤‘...")
    start_time = time.time()
    diarization = pipeline(audio_file)
    end_time = time.time()
    
    # ì²˜ë¦¬ ì™„ë£Œ í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    final_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024
    print(f"ì²˜ë¦¬ ì™„ë£Œ í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {final_memory:.1f}MB")
    print(f"ì´ ì²˜ë¦¬ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
    
    # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì¤‘ë‹¨
    stop_monitoring.set()
    
    print("\n=== í™”ì ë¶„ë¦¬ ë° STT ì²˜ë¦¬ ===")
    
    # temp í´ë” ìƒì„±
    temp_dir = "temp"
    os.makedirs(temp_dir, exist_ok=True)
    
    # Whisper ëª¨ë¸ ë¡œë”© (í•œêµ­ì–´ì— ìµœì í™”ëœ large-v3 ëª¨ë¸ ì‚¬ìš©)
    print("Whisper STT ëª¨ë¸ ë¡œë”© ì¤‘...")
    whisper_model = whisper.load_model("large-v3")
    if torch.cuda.is_available():
        # Whisper ëª¨ë¸ë„ GPUë¡œ ì´ë™
        whisper_model = whisper_model.to(device)
        print("Whisper ëª¨ë¸ì„ GPUë¡œ ì´ë™í–ˆìŠµë‹ˆë‹¤.")
    
    # ì›ë³¸ ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë”© (librosa ì‚¬ìš©)
    audio_data, sample_rate = librosa.load(audio_file, sr=None)
    
    # í˜„ì¬ ì‹œê°„ (íŒŒì¼ëª…ìš©)
    current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    print(f"\ní™”ìë³„ ìŒì„± ë¶„ë¦¬ ë° í…ìŠ¤íŠ¸ ë³€í™˜ ì‹œì‘...")
    results = []
    
    # ê° í™”ìë³„ êµ¬ê°„ì„ ì²˜ë¦¬
    for i, (turn, _, speaker) in enumerate(diarization.itertracks(yield_label=True)):
        start_sample = int(turn.start * sample_rate)
        end_sample = int(turn.end * sample_rate)
        
        # í•´ë‹¹ êµ¬ê°„ì˜ ì˜¤ë””ì˜¤ ì¶”ì¶œ
        segment_audio = audio_data[start_sample:end_sample]
        
        # íŒŒì¼ëª… ìƒì„±: {í˜„ì¬ì‹œê°„}_{í™”ìë²ˆí˜¸}_{êµ¬ê°„ë²ˆí˜¸}.wav
        filename = f"{current_time}_{speaker}_{i+1:03d}.wav"
        filepath = os.path.join(temp_dir, filename)
        
        # ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        sf.write(filepath, segment_audio, sample_rate)
        
        # Whisperë¥¼ ì‚¬ìš©í•˜ì—¬ STT ìˆ˜í–‰
        try:
            result = whisper_model.transcribe(filepath, language="ko")
            text = result["text"].strip()
            
            # ê²°ê³¼ ì €ì¥
            results.append({
                "start": turn.start,
                "end": turn.end,
                "speaker": speaker,
                "filename": filename,
                "text": text
            })
            
            print(f"[{i+1:2d}] {turn.start:.1f}s-{turn.end:.1f}s | {speaker} | {filename}")
            print(f"     í…ìŠ¤íŠ¸: {text}")
            print()
            
        except Exception as e:
            print(f"STT ì²˜ë¦¬ ì˜¤ë¥˜ ({filename}): {e}")
            results.append({
                "start": turn.start,
                "end": turn.end,
                "speaker": speaker,
                "filename": filename,
                "text": "[STT ì²˜ë¦¬ ì‹¤íŒ¨]"
            })
    
    print(f"\n=== ì²˜ë¦¬ ì™„ë£Œ ===")
    print(f"ì´ {len(results)}ê°œ êµ¬ê°„ ì²˜ë¦¬ ì™„ë£Œ")
    print(f"ë¶„ë¦¬ëœ ì˜¤ë””ì˜¤ íŒŒì¼ë“¤ì´ '{temp_dir}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # í™”ìë³„ ìš”ì•½ ì¶œë ¥
    print(f"\n=== í™”ìë³„ ë°œí™” ìš”ì•½ ===")
    speaker_texts = {}
    for result in results:
        speaker = result["speaker"]
        if speaker not in speaker_texts:
            speaker_texts[speaker] = []
        speaker_texts[speaker].append(result["text"])
    
    for speaker, texts in speaker_texts.items():
        print(f"\n[{speaker}]:")
        for i, text in enumerate(texts, 1):
            print(f"  {i}. {text}")
    
    print(f"\nì „ì²´ ëŒ€í™” ë‚´ìš©:")
    for result in results:
        print(f"{result['start']:.1f}s [{result['speaker']}]: {result['text']}")
    
    # í™”ì ì‚¬ì „ ê²€ì¦ ë° ì„ íƒì  ì„ë² ë”© ì €ì¥
    print(f"\n=== í™”ì ì‚¬ì „ ê²€ì¦ ë° ì„ íƒì  ì„ë² ë”© ì €ì¥ ===")
    verified_speakers, new_speaker_embeddings = verify_and_selective_embedding_save(pipeline, diarization, audio_file, results)
    
    # ì €ì¥ëœ ì„ë² ë”© ë°ì´í„° ìš”ì•½ ì¶œë ¥
    if new_speaker_embeddings:
        print(f"\n=== ìƒˆë¡œìš´ í™”ì ì„ë² ë”© ì €ì¥ ìš”ì•½ ===")
        for speaker, embeddings in new_speaker_embeddings.items():
            print(f"{speaker}: {len(embeddings)}ê°œ ì„ë² ë”© ë²¡í„° ì €ì¥ (ì‹ ê·œ)")
    else:
        print(f"\nğŸ“ ëª¨ë“  í™”ìê°€ ê¸°ì¡´ ë“±ë¡ í™”ìë¡œ í™•ì¸ë˜ì–´ ìƒˆë¡œìš´ ì„ë² ë”© ì €ì¥ì´ ìƒëµë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ê²€ì¦ëœ í™”ì ì •ë³´ë¡œ ê²°ê³¼ ì—…ë°ì´íŠ¸
        if verified_speakers:
            print(f"\n=== ê²€ì¦ëœ í™”ìë³„ ë°œí™” ìš”ì•½ ===")
            updated_speaker_texts = {}
            
            for result in results:
                original_speaker = result["speaker"]
                verified_info = verified_speakers.get(original_speaker, {})
                verified_name = verified_info.get('identified_as', original_speaker)
                confidence = verified_info.get('confidence', 'unknown')
                
                if verified_name not in updated_speaker_texts:
                    updated_speaker_texts[verified_name] = []
                
                # ê²°ê³¼ì— ê²€ì¦ ì •ë³´ ì¶”ê°€
                result['verified_speaker'] = verified_name
                result['verification_confidence'] = confidence
                updated_speaker_texts[verified_name].append({
                    'text': result['text'],
                    'time': f"{result['start']:.1f}s-{result['end']:.1f}s",
                    'original_label': original_speaker
                })
            
            # ê²€ì¦ëœ í™”ìë³„ ìš”ì•½ ì¶œë ¥
            for speaker_name, texts in updated_speaker_texts.items():
                confidence_info = verified_speakers.get(speaker_name, {}).get('confidence', 'unknown')
                print(f"\n[{speaker_name}] (ì‹ ë¢°ë„: {confidence_info}):")
                for i, text_info in enumerate(texts, 1):
                    print(f"  {i}. {text_info['text']} ({text_info['time']})")
            
            print(f"\n=== ê²€ì¦ëœ ì „ì²´ ëŒ€í™” ë‚´ìš© ===")
            for result in results:
                verified_speaker = result.get('verified_speaker', result['speaker'])
                confidence = result.get('verification_confidence', 'unknown')
                print(f"{result['start']:.1f}s [{verified_speaker}] ({confidence}): {result['text']}")
        
        else:
            print("í™”ì ê²€ì¦ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

except Exception as e:
    print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
    stop_monitoring.set()

print("=== í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ===")
