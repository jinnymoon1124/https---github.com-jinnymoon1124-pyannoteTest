from pyannote.audio import Pipeline
import psutil
import GPUtil
import time
import threading
import os
import whisper
import librosa
import soundfile as sf
import pickle
import json
import numpy as np
from datetime import datetime
from collections import defaultdict
from sklearn.metrics.pairwise import cosine_similarity

def monitor_resources(stop_event):
    """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ë¥¼ ëª¨ë‹ˆí„°ë§í•˜ëŠ” í•¨ìˆ˜"""
    process = psutil.Process(os.getpid())
    
    while not stop_event.is_set():
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB ë‹¨ìœ„)
        memory_info = process.memory_info()
        memory_mb = memory_info.rss / 1024 / 1024
        
        # CPU ì‚¬ìš©ë¥ 
        cpu_percent = process.cpu_percent()
        
        # GPU ì‚¬ìš©ëŸ‰ í™•ì¸
        gpus = GPUtil.getGPUs()
        if gpus:
            gpu = gpus[0]  # ì²« ë²ˆì§¸ GPU ì‚¬ìš©
            gpu_memory_used = gpu.memoryUsed
            gpu_memory_total = gpu.memoryTotal
            gpu_utilization = gpu.load * 100
            gpu_temp = gpu.temperature if hasattr(gpu, 'temperature') else 'N/A'
            print(f"[ë¦¬ì†ŒìŠ¤] ë©”ëª¨ë¦¬: {memory_mb:.1f}MB | CPU: {cpu_percent:.1f}% | GPU ë©”ëª¨ë¦¬: {gpu_memory_used}MB/{gpu_memory_total}MB | GPU ì‚¬ìš©ë¥ : {gpu_utilization:.1f}% | GPU ì˜¨ë„: {gpu_temp}Â°C")
        else:
            print(f"[ë¦¬ì†ŒìŠ¤] ë©”ëª¨ë¦¬: {memory_mb:.1f}MB | CPU: {cpu_percent:.1f}% | GPU: ê°ì§€ë˜ì§€ ì•ŠìŒ")
        
        time.sleep(2)  # 2ì´ˆë§ˆë‹¤ ëª¨ë‹ˆí„°ë§

def extract_and_save_embeddings(pipeline, diarization, audio_file, results):
    """
    í™”ìë³„ ì„ë² ë”© ë²¡í„°ë¥¼ ì¶”ì¶œí•˜ê³  ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    """
    print("\n=== í™”ì ì„ë² ë”© ì¶”ì¶œ ë° ì €ì¥ ===")
    
    # ì„ë² ë”© ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    embeddings_dir = "embeddings"
    os.makedirs(embeddings_dir, exist_ok=True)
    
    # í™”ìë³„ ì„ë² ë”© ì €ì¥ì†Œ
    speaker_embeddings = defaultdict(list)
    
    try:
        # ì„ë² ë”© ëª¨ë¸ ì¶”ì¶œ (íŒŒì´í”„ë¼ì¸ ë‚´ë¶€ì˜ ì„ë² ë”© ëª¨ë¸ ì ‘ê·¼)
        embedding_model = pipeline._embedding
        print(f"ì„ë² ë”© ëª¨ë¸: {type(embedding_model)}")
        
        # ì˜¤ë””ì˜¤ ë°ì´í„° ë¡œë“œ
        audio_data, sample_rate = librosa.load(audio_file, sr=16000)
        current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        print(f"ì„ë² ë”© ì¶”ì¶œ ì‹œì‘... (ì´ {len(results)}ê°œ ì„¸ê·¸ë¨¼íŠ¸)")
        
        for idx, result in enumerate(results):
            speaker = result["speaker"]
            start_time = result["start"]
            end_time = result["end"]
            
            # í•´ë‹¹ êµ¬ê°„ì˜ ì˜¤ë””ì˜¤ ì¶”ì¶œ
            start_sample = int(start_time * sample_rate)
            end_sample = int(end_time * sample_rate)
            segment_audio = audio_data[start_sample:end_sample]
            
            # ë„ˆë¬´ ì§§ì€ ì„¸ê·¸ë¨¼íŠ¸ëŠ” ê±´ë„ˆë›°ê¸° (0.5ì´ˆ ë¯¸ë§Œ)
            if len(segment_audio) < sample_rate * 0.5:
                print(f"ì„¸ê·¸ë¨¼íŠ¸ ë„ˆë¬´ ì§§ìŒ, ê±´ë„ˆë›°ê¸°: {speaker} ({start_time:.1f}s-{end_time:.1f}s)")
                continue
            
            try:
                # ì§ì ‘ WeSpeaker ëª¨ë¸ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ì¶”ì¶œ (íŒŒì¼ ì—†ì´ ë©”ëª¨ë¦¬ì—ì„œ ì§ì ‘)
                from pyannote.audio import Model
                
                # ë‚´ë¶€ ëª¨ë¸ì— ì§ì ‘ ì ‘ê·¼ ë˜ëŠ” ìƒˆë¡œ ë¡œë“œ
                if hasattr(embedding_model, 'model_'):
                    direct_model = embedding_model.model_
                else:
                    # WeSpeaker ëª¨ë¸ ì§ì ‘ ë¡œë“œ
                    model_path = "models/pyannote_model_wespeaker-voxceleb-resnet34-LM.bin"
                    direct_model = Model.from_pretrained(model_path)
                    if torch.cuda.is_available():
                        direct_model = direct_model.cuda()
                
                # ì˜¬ë°”ë¥¸ í…ì„œ ì°¨ì›ìœ¼ë¡œ ë³€í™˜: (batch, channels, samples)
                audio_tensor = torch.from_numpy(segment_audio).float()
                audio_tensor = audio_tensor.unsqueeze(0).unsqueeze(0)  # (1, 1, samples)
                
                if torch.cuda.is_available():
                    audio_tensor = audio_tensor.cuda()
                
                # ì„ë² ë”© ì¶”ì¶œ
                with torch.no_grad():
                    embedding = direct_model(audio_tensor)
                
                # GPU tensorë¥¼ CPU numpyë¡œ ë³€í™˜
                embedding_vector = embedding.cpu().numpy().flatten()
                
                # í™”ìë³„ ì„ë² ë”© ì €ì¥
                speaker_embeddings[speaker].append({
                    'embedding': embedding_vector,
                    'start_time': start_time,
                    'end_time': end_time,
                    'duration': end_time - start_time,
                    'text': result['text'],
                    'filename': result['filename'],
                    'timestamp': current_time
                })
                
                print(f"[{idx+1:2d}/{len(results)}] ì„ë² ë”© ì¶”ì¶œ: {speaker} ({start_time:.1f}s-{end_time:.1f}s) - ì°¨ì›: {embedding_vector.shape}")
                
            except Exception as e:
                print(f"ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨ ({speaker}, {idx}): {e}")
        
        # í™”ìë³„ ì„ë² ë”© ë°ì´í„° ì €ì¥
        print(f"\nì„ë² ë”© ë°ì´í„° ì €ì¥ ì¤‘...")
        total_embeddings = 0
        
        for speaker, embeddings in speaker_embeddings.items():
            if not embeddings:
                continue
                
            # ê°œë³„ ì„ë² ë”© íŒŒì¼ ì €ì¥
            embedding_file = os.path.join(embeddings_dir, f"{current_time}_{speaker}_embeddings.pkl")
            with open(embedding_file, 'wb') as f:
                pickle.dump(embeddings, f)
            
            # í‰ê·  ì„ë² ë”© ê³„ì‚° (í™”ì ëŒ€í‘œ ë²¡í„°)
            all_embeddings = np.array([emb['embedding'] for emb in embeddings])
            mean_embedding = np.mean(all_embeddings, axis=0)
            std_embedding = np.std(all_embeddings, axis=0)
            
            # í™”ì í”„ë¡œíŒŒì¼ ì €ì¥
            speaker_profile = {
                'speaker_id': speaker,
                'mean_embedding': mean_embedding,
                'std_embedding': std_embedding,
                'num_segments': len(embeddings),
                'total_duration': sum([emb['duration'] for emb in embeddings]),
                'timestamp': current_time,
                'audio_file': audio_file,
                'embedding_dim': mean_embedding.shape[0],
                'sample_embeddings': embeddings[:3] if len(embeddings) > 3 else embeddings  # ìƒ˜í”Œ ì €ì¥
            }
            
            profile_file = os.path.join(embeddings_dir, f"{current_time}_{speaker}_profile.pkl")
            with open(profile_file, 'wb') as f:
                pickle.dump(speaker_profile, f)
            
            total_embeddings += len(embeddings)
            print(f"âœ… {speaker} í”„ë¡œíŒŒì¼ ì €ì¥: {profile_file}")
            print(f"   - ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜: {len(embeddings)}")
            print(f"   - ì´ ë°œí™” ì‹œê°„: {speaker_profile['total_duration']:.2f}ì´ˆ")
            print(f"   - ì„ë² ë”© ì°¨ì›: {mean_embedding.shape[0]}")
        
        # ì „ì²´ ì„¸ì…˜ ë©”íƒ€ë°ì´í„° ì €ì¥
        session_metadata = {
            'timestamp': current_time,
            'audio_file': audio_file,
            'speakers': list(speaker_embeddings.keys()),
            'total_segments': len(results),
            'total_embeddings': total_embeddings,
            'embedding_dim': mean_embedding.shape[0] if speaker_embeddings else 0,
            'sample_rate': sample_rate
        }
        
        metadata_file = os.path.join(embeddings_dir, f"{current_time}_session_metadata.json")
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(session_metadata, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ‰ ì„ë² ë”© ë°ì´í„° ì €ì¥ ì™„ë£Œ!")
        print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {embeddings_dir}/")
        print(f"ğŸ“Š ì´ ì„ë² ë”© ìˆ˜: {total_embeddings}ê°œ")
        print(f"ğŸ‘¥ í™”ì ìˆ˜: {len(speaker_embeddings)}ëª…")
        
        return speaker_embeddings, session_metadata
        
    except Exception as e:
        print(f"âŒ ì„ë² ë”© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {}, {}

def load_speaker_embeddings(embeddings_dir="embeddings"):
    """ì €ì¥ëœ í™”ì ì„ë² ë”© ë°ì´í„° ë¡œë“œ"""
    import glob
    
    profiles = {}
    profile_files = glob.glob(f"{embeddings_dir}/*_profile.pkl")
    
    print(f"\n=== ì €ì¥ëœ í™”ì í”„ë¡œíŒŒì¼ ë¡œë“œ ===")
    print(f"ë°œê²¬ëœ í”„ë¡œíŒŒì¼ íŒŒì¼: {len(profile_files)}ê°œ")
    
    for profile_file in profile_files:
        try:
            with open(profile_file, 'rb') as f:
                profile = pickle.load(f)
                speaker_id = profile['speaker_id']
                profiles[speaker_id] = profile
                print(f"âœ… ë¡œë“œ: {speaker_id} ({profile['num_segments']}ê°œ ì„¸ê·¸ë¨¼íŠ¸)")
        except Exception as e:
            print(f"âŒ í”„ë¡œíŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({profile_file}): {e}")
    
    return profiles

def identify_speaker(new_embedding, known_profiles, threshold=0.8):
    """ìƒˆë¡œìš´ ì„ë² ë”©ê³¼ ê¸°ì¡´ í™”ì í”„ë¡œíŒŒì¼ì„ ë¹„êµí•˜ì—¬ í™”ì ì‹ë³„"""
    if not known_profiles:
        return "UNKNOWN", 0.0
    
    best_match = None
    best_similarity = 0
    
    # ì…ë ¥ ì„ë² ë”©ì„ 2D ë°°ì—´ë¡œ ë³€í™˜ (cosine_similarity ìš”êµ¬ì‚¬í•­)
    if new_embedding.ndim == 1:
        new_embedding = new_embedding.reshape(1, -1)
    
    for speaker_id, profile in known_profiles.items():
        mean_embedding = profile['mean_embedding'].reshape(1, -1)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarity = cosine_similarity(new_embedding, mean_embedding)[0][0]
        
        if similarity > best_similarity:
            best_similarity = similarity
            best_match = speaker_id
    
    if best_similarity > threshold:
        return best_match, best_similarity
    else:
        return "UNKNOWN", best_similarity

print("=== í™”ì ë¶„ë¦¬ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ===")
print("ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

# ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘
stop_monitoring = threading.Event()
monitor_thread = threading.Thread(target=monitor_resources, args=(stop_monitoring,))
monitor_thread.daemon = True
monitor_thread.start()

# ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ ì¶œë ¥
initial_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024
print(f"ì´ˆê¸° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {initial_memory:.1f}MB")

try:
    # yaml ë¶ˆëŸ¬ì˜¤ê¸°
    print("íŒŒì´í”„ë¼ì¸ ë¡œë”© ì¤‘...")
    pipeline = Pipeline.from_pretrained("models/pyannote_diarization_config.yaml")
    
    # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ ë° íŒŒì´í”„ë¼ì¸ì„ GPUë¡œ ì´ë™
    import torch
    if torch.cuda.is_available():
        device = torch.device("cuda")
        pipeline.to(device)
        print(f"GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
        print(f"íŒŒì´í”„ë¼ì¸ì„ GPUë¡œ ì´ë™í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    
    # íŒŒì´í”„ë¼ì¸ ë¡œë”© í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    after_load_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024
    print(f"íŒŒì´í”„ë¼ì¸ ë¡œë”© í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {after_load_memory:.1f}MB (ì¦ê°€ëŸ‰: {after_load_memory - initial_memory:.1f}MB)")

    # ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
    audio_file = "audio/test.wav"  # í…ŒìŠ¤íŠ¸í•  ìŒì„± íŒŒì¼

    # í™”ì ë¶„ë¦¬ ì‹¤í–‰
    print("í™”ì ë¶„ë¦¬ ì²˜ë¦¬ ì¤‘...")
    start_time = time.time()
    diarization = pipeline(audio_file)
    end_time = time.time()
    
    # ì²˜ë¦¬ ì™„ë£Œ í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    final_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024
    print(f"ì²˜ë¦¬ ì™„ë£Œ í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {final_memory:.1f}MB")
    print(f"ì´ ì²˜ë¦¬ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
    
    # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì¤‘ë‹¨
    stop_monitoring.set()
    
    print("\n=== í™”ì ë¶„ë¦¬ ë° STT ì²˜ë¦¬ ===")
    
    # temp í´ë” ìƒì„±
    temp_dir = "temp"
    os.makedirs(temp_dir, exist_ok=True)
    
    # Whisper ëª¨ë¸ ë¡œë”© (í•œêµ­ì–´ì— ìµœì í™”ëœ large-v3 ëª¨ë¸ ì‚¬ìš©)
    print("Whisper STT ëª¨ë¸ ë¡œë”© ì¤‘...")
    whisper_model = whisper.load_model("large-v3")
    if torch.cuda.is_available():
        # Whisper ëª¨ë¸ë„ GPUë¡œ ì´ë™
        whisper_model = whisper_model.to(device)
        print("Whisper ëª¨ë¸ì„ GPUë¡œ ì´ë™í–ˆìŠµë‹ˆë‹¤.")
    
    # ì›ë³¸ ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë”© (librosa ì‚¬ìš©)
    audio_data, sample_rate = librosa.load(audio_file, sr=None)
    
    # í˜„ì¬ ì‹œê°„ (íŒŒì¼ëª…ìš©)
    current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    print(f"\ní™”ìë³„ ìŒì„± ë¶„ë¦¬ ë° í…ìŠ¤íŠ¸ ë³€í™˜ ì‹œì‘...")
    results = []
    
    # ê° í™”ìë³„ êµ¬ê°„ì„ ì²˜ë¦¬
    for i, (turn, _, speaker) in enumerate(diarization.itertracks(yield_label=True)):
        start_sample = int(turn.start * sample_rate)
        end_sample = int(turn.end * sample_rate)
        
        # í•´ë‹¹ êµ¬ê°„ì˜ ì˜¤ë””ì˜¤ ì¶”ì¶œ
        segment_audio = audio_data[start_sample:end_sample]
        
        # íŒŒì¼ëª… ìƒì„±: {í˜„ì¬ì‹œê°„}_{í™”ìë²ˆí˜¸}_{êµ¬ê°„ë²ˆí˜¸}.wav
        filename = f"{current_time}_{speaker}_{i+1:03d}.wav"
        filepath = os.path.join(temp_dir, filename)
        
        # ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        sf.write(filepath, segment_audio, sample_rate)
        
        # Whisperë¥¼ ì‚¬ìš©í•˜ì—¬ STT ìˆ˜í–‰
        try:
            result = whisper_model.transcribe(filepath, language="ko")
            text = result["text"].strip()
            
            # ê²°ê³¼ ì €ì¥
            results.append({
                "start": turn.start,
                "end": turn.end,
                "speaker": speaker,
                "filename": filename,
                "text": text
            })
            
            print(f"[{i+1:2d}] {turn.start:.1f}s-{turn.end:.1f}s | {speaker} | {filename}")
            print(f"     í…ìŠ¤íŠ¸: {text}")
            print()
            
        except Exception as e:
            print(f"STT ì²˜ë¦¬ ì˜¤ë¥˜ ({filename}): {e}")
            results.append({
                "start": turn.start,
                "end": turn.end,
                "speaker": speaker,
                "filename": filename,
                "text": "[STT ì²˜ë¦¬ ì‹¤íŒ¨]"
            })
    
    print(f"\n=== ì²˜ë¦¬ ì™„ë£Œ ===")
    print(f"ì´ {len(results)}ê°œ êµ¬ê°„ ì²˜ë¦¬ ì™„ë£Œ")
    print(f"ë¶„ë¦¬ëœ ì˜¤ë””ì˜¤ íŒŒì¼ë“¤ì´ '{temp_dir}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # í™”ìë³„ ìš”ì•½ ì¶œë ¥
    print(f"\n=== í™”ìë³„ ë°œí™” ìš”ì•½ ===")
    speaker_texts = {}
    for result in results:
        speaker = result["speaker"]
        if speaker not in speaker_texts:
            speaker_texts[speaker] = []
        speaker_texts[speaker].append(result["text"])
    
    for speaker, texts in speaker_texts.items():
        print(f"\n[{speaker}]:")
        for i, text in enumerate(texts, 1):
            print(f"  {i}. {text}")
    
    print(f"\nì „ì²´ ëŒ€í™” ë‚´ìš©:")
    for result in results:
        print(f"{result['start']:.1f}s [{result['speaker']}]: {result['text']}")
    
    # ì„ë² ë”© ì¶”ì¶œ ë° ì €ì¥
    speaker_embeddings, session_metadata = extract_and_save_embeddings(pipeline, diarization, audio_file, results)
    
    # ì €ì¥ëœ ì„ë² ë”© ë°ì´í„° ìš”ì•½ ì¶œë ¥
    if speaker_embeddings:
        print(f"\n=== ì„ë² ë”© ì €ì¥ ìš”ì•½ ===")
        for speaker, embeddings in speaker_embeddings.items():
            print(f"{speaker}: {len(embeddings)}ê°œ ì„ë² ë”© ë²¡í„° ì €ì¥")
        
        # ì„ë² ë”© í™œìš© ì˜ˆì‹œ (ê¸°ì¡´ í™”ì í”„ë¡œíŒŒì¼ ë¡œë“œ í…ŒìŠ¤íŠ¸)
        print(f"\n=== ì €ì¥ëœ í”„ë¡œíŒŒì¼ ë¡œë“œ í…ŒìŠ¤íŠ¸ ===")
        loaded_profiles = load_speaker_embeddings()
        
        if loaded_profiles:
            print(f"ë¡œë“œëœ í™”ì í”„ë¡œíŒŒì¼: {list(loaded_profiles.keys())}")
            
            # í™”ì ì‹ë³„ í…ŒìŠ¤íŠ¸ (ì²« ë²ˆì§¸ ì„ë² ë”©ìœ¼ë¡œ í…ŒìŠ¤íŠ¸)
            if results:
                first_speaker = results[0]['speaker']
                if first_speaker in speaker_embeddings and speaker_embeddings[first_speaker]:
                    test_embedding = speaker_embeddings[first_speaker][0]['embedding']
                    identified_speaker, similarity = identify_speaker(test_embedding, loaded_profiles, threshold=0.7)
                    print(f"í…ŒìŠ¤íŠ¸ ì„ë² ë”© ì‹ë³„ ê²°ê³¼: {identified_speaker} (ìœ ì‚¬ë„: {similarity:.3f})")
        else:
            print("ë¡œë“œëœ í”„ë¡œíŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

except Exception as e:
    print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
    stop_monitoring.set()

print("=== í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ===")
